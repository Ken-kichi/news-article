

ãªãœä½œã£ãŸã‹ï¼Ÿ
- ãƒ‹ãƒ¥ãƒ¼ã‚¹è§£èª¬è¨˜äº‹ã‚’éŸ³å£°ã«ã—ã¦å®¶äº‹ã‚’ã—ãªãŒã‚‰èããŸã‹ã£ãŸ
- æ¯æ—¥æŠ•ç¨¿ã—ã¦ã„ã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹è§£èª¬è¨˜äº‹ã‚’å‹•ç”»ã«ã—ãŸã‚‰å‹•ç”»ã§ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’è¦‹ã‚‹å±¤ã«ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒã§ãã‚‹
- LangGraphã§æ–‡ç« ç”Ÿæˆä»¥å¤–ã®ç”¨é€”ã«æŒ‘æˆ¦ã—ã¦ã¿ãŸã‹ã£ãŸ

å­¦ç¿’ã§ãã‚‹ã¨ã“ã‚
LangGraphã®å­¦ç¿’
å‹•ç”»ç”Ÿæˆ
éŸ³å£°ç”Ÿæˆ
å‹•ç”»ç·¨é›†
Youtubeã‚¢ãƒƒãƒ—
Canvaã®ä½¿ã„æ–¹

## ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
### ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚©ãƒ«ãƒ€ã®ä½œæˆ
```bash
mkdir news-article
cd news-article
uv init
uv venv
uv add azure-cognitiveservices-speech dotenv langchain langgraph openai langchain_openai moviepy typer
mkdir article movie output
touch config.py state.py nodes.py graph.py
```

movieã¯ä»¥ä¸‹ã®ã‚µã‚¤ãƒˆã‹ã‚‰å–å¾—ã—ã¾ã™ã€‚


## Azureã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
### Azure AI Foundry
eastusã§ä½œæˆ
FLUX.1-Kontext-proã€€ã¨ã€€gpt-4.1ã€€ã‚’ä½œæˆ
### Microsoft Foundry | éŸ³å£°ã‚µãƒ¼ãƒ“ã‚¹
East USã§ä½œæˆ

https://learn.microsoft.com/ja-jp/azure/ai-services/speech-service/get-started-text-to-speech?tabs=new-foundry%2Cmacos&pivots=programming-language-python

## config.pyã®è¨­å®š
### .envã®ä½œæˆ
```txt
# ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
AZURE_TEXT_API_KEY=
AZURE_TEXT_ENDPOINT=

# éŸ³å£°
AZURE_SPEECH_KEY=
AZURE_SPEECH_ENDPOINT=
AZURE_SPEECH_REGION=

# ç”»åƒç”Ÿæˆ
AZURE_IMAGE_KEY=
AZURE_IMAGE_ENDPOINT=

```
### config.pyã®ä½œæˆ
```py:config.py
import os
from dotenv import load_dotenv

load_dotenv()


def _split_endpoint(endpoint: str | None) -> tuple[str | None, str | None]:
    """Return (resource_base_url, api_version) extracted from a raw endpoint."""
    if not endpoint:
        return None, None

    base = endpoint.strip()
    if not base:
        return None, None

    api_version = None
    if "api-version=" in base:
        api_version = base.split("api-version=")[-1].split("&")[0].strip()
    base = base.split("?")[0]
    if "/openai/" in base:
        base = base.split("/openai/")[0]

    normalized = base.rstrip("/") + "/"
    return normalized, api_version


_raw_text_endpoint = os.getenv("AZURE_TEXT_ENDPOINT")
_text_endpoint, _text_version = _split_endpoint(_raw_text_endpoint)

_raw_image_endpoint = os.getenv("AZURE_IMAGE_ENDPOINT")
_image_endpoint, _image_version = _split_endpoint(_raw_image_endpoint)


class Config:
    # Azure OpenAI text
    AZURE_TEXT_API_KEY = os.getenv("AZURE_TEXT_API_KEY")
    AZURE_TEXT_ENDPOINT = _text_endpoint
    AZURE_TEXT_API_VERSION = os.getenv("AZURE_TEXT_API_VERSION", _text_version or "2024-02-01-preview")
    AZURE_OPENAI_DEPLOYMENT_NAME = "gpt-4.1"

    # Azure AI Speech
    AZURE_SPEECH_KEY = os.getenv("AZURE_SPEECH_KEY")
    AZURE_SPEECH_ENDPOINT = os.getenv("AZURE_SPEECH_ENDPOINT")
    AZURE_SPEECH_REGION = os.getenv("AZURE_SPEECH_REGION")

    # Azure Image
    AZURE_IMAGE_API_KEY = os.getenv("AZURE_IMAGE_KEY")
    AZURE_IMAGE_ENDPOINT = _image_endpoint
    AZURE_IMAGE_API_VERSION = os.getenv("AZURE_IMAGE_API_VERSION", _image_version or "2023-12-01-preview")
    AZURE_IMAGE_DEVELOPMENT_NAME = "FLUX.1-Kontext-pro"

    ARTICLE_DIR = "./article"
    OUTPUT_DIR = "./output"
    JP_FONT_PATH = os.getenv("JP_FONT_PATH")
    MOVIE_DIR = "./movie"

```
## state.pyã®ä½œæˆ
```py:state.py
from typing import TypedDict


class ArticleData(TypedDict):
    title: str
    display_title: str
    content: str
    date: str


class AgentState(TypedDict):
    start_date: str
    end_date: str
    run_output_dir: str
    articles: list[ArticleData]
    image_prompts: list[str]
    audio_paths: list[str]
    image_paths: list[str]
    script_paths: list[str]
    video_path: str | None
    error: str | None

```
## nodes.pyã®ä½œæˆ
```py:nodes.py
import base64
import json
import os
import random
import re
from datetime import datetime
import azure.cognitiveservices.speech as speechsdk
from openai import AzureOpenAI
from moviepy.video.VideoClip import ImageClip
from moviepy.audio.io.AudioFileClip import AudioFileClip
from moviepy.video.compositing.CompositeVideoClip import concatenate_videoclips
from moviepy.video.fx.Resize import Resize
from moviepy.video.io.VideoFileClip import VideoFileClip
from config import Config
from state import AgentState


def _log_node_output(run_dir: str, node_name: str, payload: dict):
    os.makedirs(run_dir, exist_ok=True)
    log_path = os.path.join(run_dir, "node_logs.jsonl")
    entry = {
        "timestamp": datetime.now().isoformat(timespec="seconds"),
        "node": node_name,
        "payload": payload
    }
    with open(log_path, "a", encoding="utf-8") as log_file:
        json.dump(entry, log_file, ensure_ascii=False)
        log_file.write("\n")


def _extract_title_from_content(raw_content: str, fallback: str) -> str:
    for line in raw_content.splitlines():
        candidate = line.strip().lstrip("#").strip()
        if candidate:
            return candidate[:120]
    return fallback


text_client = AzureOpenAI(
    api_key=Config.AZURE_TEXT_API_KEY,
    api_version=Config.AZURE_TEXT_API_VERSION,
    azure_endpoint=Config.AZURE_TEXT_ENDPOINT,
    azure_deployment=Config.AZURE_OPENAI_DEPLOYMENT_NAME,
)

image_client = AzureOpenAI(
    api_key=Config.AZURE_IMAGE_API_KEY,
    api_version=Config.AZURE_IMAGE_API_VERSION,
    azure_endpoint=Config.AZURE_IMAGE_ENDPOINT,
    azure_deployment=Config.AZURE_IMAGE_DEVELOPMENT_NAME,
)

VIDEO_EXTENSIONS = (".mp4", ".mov", ".m4v", ".avi", ".webm", ".mkv")


def _list_movie_files() -> list[str]:
    movie_dir = Config.MOVIE_DIR
    if not movie_dir or not os.path.isdir(movie_dir):
        return []
    files = []
    for name in os.listdir(movie_dir):
        if name.lower().endswith(VIDEO_EXTENSIONS):
            files.append(os.path.join(movie_dir, name))
    return files


def fetch_articles_node(state: AgentState):
    """articleãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰è¨˜äº‹ã‚’å–å¾—ã—ã€ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã«è¦ç´„ãƒ»æ•´å½¢ã™ã‚‹"""
    target_articles = []
    start = datetime.strptime(state['start_date'], "%Y%m%d")
    end = datetime.strptime(state['end_date'], "%Y%m%d")
    run_dir = state.get('run_output_dir') or Config.OUTPUT_DIR

    if not os.path.exists(Config.ARTICLE_DIR):
        os.makedirs(Config.ARTICLE_DIR)

    # 1. ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    files_to_process = []
    for filename in os.listdir(Config.ARTICLE_DIR):
        match = re.match(r"(\d{8})_(.*)\.md", filename)
        if match:
            file_date_str, title = match.groups()
            file_date = datetime.strptime(file_date_str, "%Y%m%d")
            if start <= file_date <= end:
                files_to_process.append((filename, title, file_date_str))

    # 2. å„è¨˜äº‹ã®èª­ã¿è¾¼ã¿ã¨è¦ç´„ï¼ˆãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åŸç¨¿ä½œæˆï¼‰
    for filename, title, date_str in files_to_process:
        with open(os.path.join(Config.ARTICLE_DIR, filename), 'r', encoding='utf-8') as f:
            raw_content = f.read()

        # GPT-4oã«ã‚ˆã‚‹è¦ç´„ã¨ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•´å½¢
        # ã“ã“ã§URLã®é™¤å»ã‚„è‡ªç„¶ãªè¨€ã„å›ã—ã¸ã®å¤‰æ›ã‚’æŒ‡ç¤º
        response = text_client.chat.completions.create(
            model=Config.AZURE_OPENAI_DEPLOYMENT_NAME,  # GPT-4oç”¨ãƒ‡ãƒ—ãƒ­ã‚¤å
            messages=[
                {"role": "system", "content": "ã‚ãªãŸã¯å„ªç§€ãªãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¢ãƒŠã‚¦ãƒ³ã‚µãƒ¼ã§ã™ã€‚"},
                {"role": "user", "content": f"""
ä»¥ä¸‹ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’ã€YouTubeã‚·ãƒ§ãƒ¼ãƒˆç”¨ã®ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åŸç¨¿ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚

ã€åˆ¶ç´„äº‹é …ã€‘
ãƒ»500æ–‡å­—ä»¥å†…ã€‚
ãƒ»URLã‚„è¨˜å·ï¼ˆURLã€[ ]ã€( )ãªã©ï¼‰ã¯èª­ã¿ä¸Šã’ã«é©ã•ãªã„ãŸã‚ã€å®Œå…¨ã«å‰Šé™¤ã¾ãŸã¯è‡ªç„¶ãªè¨€è‘‰ã«ç½®ãæ›ãˆã‚‹ã“ã¨ã€‚
ãƒ»è¦–è´è€…ãŒèãå–ã‚Šã‚„ã™ã„ã‚ˆã†ã€å°‚é–€ç”¨èªã¯é¿ã‘ã€è‡ªç„¶ãªè©±ã—è¨€è‘‰ï¼ˆã§ã™ãƒ»ã¾ã™èª¿ï¼‰ã«ã™ã‚‹ã“ã¨ã€‚

è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«: {title}
è¨˜äº‹å†…å®¹:
{raw_content}
"""}
            ]
        )

        summarized_content = response.choices[0].message.content.strip()
        human_title = _extract_title_from_content(
            raw_content, title.replace("_", " "))

        target_articles.append({
            'title': title,
            'display_title': human_title,
            'content': summarized_content,  # ã“ã“ã«ç¶ºéº—ãªè¦ç´„ãŒå…¥ã‚‹
            'date': date_str
        })
        print(f"âœ… è¦ç´„å®Œäº†: {title}")

    _log_node_output(
        run_dir,
        "fetch_articles",
        {
            "article_count": len(target_articles),
            "article_titles": [article['display_title'] for article in target_articles],
            "articles": target_articles
        }
    )

    return {'articles': target_articles, 'run_output_dir': run_dir}


def generate_assets_node(state: AgentState):
    audio_paths = []
    image_paths = []
    image_prompts = []
    script_paths = []
    voice_outputs = []
    image_outputs = []

    run_dir = state.get('run_output_dir') or Config.OUTPUT_DIR
    os.makedirs(run_dir, exist_ok=True)

    for i, article in enumerate(state['articles']):
        speech_config = speechsdk.SpeechConfig(
            subscription=Config.AZURE_SPEECH_KEY,
            region=Config.AZURE_SPEECH_REGION
        )
        speech_config.speech_synthesis_voice_name = "ja-JP-NanamiNeural"

        audio_filename = f"audio_{i}.wav"
        audio_path = os.path.join(run_dir, audio_filename)
        audio_config = speechsdk.audio.AudioOutputConfig(filename=audio_path)

        synthesizer = speechsdk.SpeechSynthesizer(
            speech_config=speech_config, audio_config=audio_config)
        synthesizer.speak_text_async(article['content']).get()
        audio_paths.append(audio_path)

        script_filename = f"script_{i}.txt"
        script_path = os.path.join(run_dir, script_filename)
        with open(script_path, "w", encoding="utf-8") as script_file:
            script_file.write(article['content'])
        script_paths.append(script_path)

        voice_outputs.append({
            "index": i,
            "article_title": article.get('display_title') or article['title'],
            "audio_path": audio_path,
            "script_path": script_path,
            "spoken_text": article['content']
        })

        # 1. GPT-4oã«ã€Œé¢¨æ™¯ã€ã¨ã—ã¦ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æå†™ã•ã›ã‚‹
        prompt_response = text_client.chat.completions.create(
            model=Config.AZURE_OPENAI_DEPLOYMENT_NAME,
            messages=[
                {
                    "role": "system",
                    "content": (
                        "ã‚ãªãŸã¯æ˜ ç”»ã®ã‚³ãƒ³ã‚»ãƒ—ãƒˆã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆã§ã™ã€‚ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’èª­ã¿ã€"
                        "ãã®èƒŒæ™¯ã‚’è±¡å¾´ã™ã‚‹ã‚ˆã†ãªã€ç¾ã—ãåºƒå¤§ãªé¢¨æ™¯ã€ã®ç”»åƒç”Ÿæˆç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è‹±èªã§ä½œæˆã—ã¦ãã ã•ã„ã€‚"
                        "äººç‰©ã‚’ç›´æ¥æãã®ã§ã¯ãªãã€ãã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒèµ·ãã¦ã„ã‚‹å ´æ‰€ã‚„ã€"
                        "ãã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒã‚‚ãŸã‚‰ã™é›°å›²æ°—ã‚’è±¡å¾´ã™ã‚‹é¢¨æ™¯ã‚’æå†™ã—ã¦ãã ã•ã„ã€‚"
                    )
                },
                {
                    "role": "user",
                    "content": f"è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«: {article.get('display_title') or article['title']}\nè¨˜äº‹å†…å®¹: {article['content']}"
                }
            ]
        )
        img_prompt = prompt_response.choices[0].message.content.strip()

        image_prompts.append(img_prompt)

        image_result = image_client.images.generate(
            model=Config.AZURE_IMAGE_DEVELOPMENT_NAME,
            prompt=f"{img_prompt} Digital art style, vibrant colors, 9:16 aspect ratio focus.",
            size="1792x1024",
            n=1,
            response_format="b64_json",
        )

        image_data = image_result.data[0]
        image_location = None
        if image_data.url:
            image_location = image_data.url
            image_paths.append(image_location)
        else:
            image_filename = f"image_{i}.png"
            image_path = os.path.join(run_dir, image_filename)
            with open(image_path, "wb") as img_file:
                img_file.write(base64.b64decode(image_data.b64_json))
            image_location = image_path
            image_paths.append(image_location)

        image_outputs.append({
            "index": i,
            "article_title": article.get('display_title') or article['title'],
            "prompt": img_prompt,
            "image_path": image_location
        })

    _log_node_output(
        run_dir,
        "generate_assets",
        {
            "audio_files": audio_paths,
            "image_files": image_paths,
            "script_files": script_paths,
            "voice_outputs": voice_outputs,
            "image_prompts": image_prompts,
            "image_outputs": image_outputs
        }
    )

    return {
        'audio_paths': audio_paths,
        'image_paths': image_paths,
        'image_prompts': image_prompts,
        'script_paths': script_paths,
        'run_output_dir': run_dir
    }


def create_short_video_node(state: AgentState):
    clips = []
    run_dir = state.get('run_output_dir') or Config.OUTPUT_DIR
    os.makedirs(run_dir, exist_ok=True)
    output_path = os.path.join(run_dir, "final_youtube_short.mp4")
    movie_files = _list_movie_files()
    video_sources: list[VideoFileClip] = []
    article_visual_logs = []

    for i, article in enumerate(state['articles']):
        audio = AudioFileClip(state['audio_paths'][i])
        duration = audio.duration or 0
        duration = max(duration, 0.001)

        base_image = ImageClip(state['image_paths'][i], duration=duration)
        base_image = base_image.with_effects([Resize(height=1920)])

        def zoom_factor(t, total=duration):
            return 1 + 0.05 * (t / max(total, 0.001))

        base_image = base_image.with_effects([Resize(new_size=zoom_factor)])
        base_image = base_image.with_position("center")

        segments = []
        movie_segments_log = []

        image_intro_duration = min(5, duration)
        segments.append(base_image.with_duration(image_intro_duration))
        remaining = duration - image_intro_duration

        while remaining > 1e-3 and movie_files:
            movie_path = random.choice(movie_files)
            try:
                video_clip = VideoFileClip(movie_path)
                video_sources.append(video_clip)
            except Exception:
                continue

            clip_duration = min(5, remaining, video_clip.duration or 0)
            if clip_duration <= 0:
                video_clip.close()
                continue

            max_start = max(
                0, (video_clip.duration or clip_duration) - clip_duration)
            start = random.uniform(0, max_start) if max_start > 0 else 0

            segment = video_clip.subclipped(
                start_time=start,
                end_time=start + clip_duration
            ).with_audio(None)

            segment = segment.with_effects(
                [Resize(height=1920)]).with_position("center")
            segments.append(segment)

            movie_segments_log.append({
                "file": os.path.basename(movie_path),
                "start": round(start, 2),
                "duration": round(clip_duration, 2)
            })

            remaining -= clip_duration

        if remaining > 1e-3:
            segments.append(base_image.with_duration(remaining))

        article_video = concatenate_videoclips(
            segments, method="compose").with_duration(duration)
        article_video.audio = audio
        clips.append(article_video)

        article_visual_logs.append({
            "article": article.get('display_title') or article['title'],
            "movie_segments": movie_segments_log
        })

    final_video = concatenate_videoclips(clips, method="compose")

    try:
        final_video.write_videofile(
            output_path, fps=24, codec="libx264", audio_codec="aac"
        )
    finally:
        final_video.close()
        for source in video_sources:
            try:
                source.close()
            except Exception:
                pass

    _log_node_output(
        run_dir,
        "create_video",
        {
            "video_path": output_path,
            "clip_count": len(clips),
            "articles": article_visual_logs
        }
    )

    return {"video_path": output_path, 'run_output_dir': run_dir}

```
## graph.pyã®ä½œæˆ
```py:graph.py
from langgraph.graph import StateGraph, END
from state import AgentState
from nodes import fetch_articles_node, generate_assets_node, create_short_video_node


def create_graph():
    workflow = StateGraph(AgentState)

    # ãƒãƒ¼ãƒ‰ã®ç™»éŒ²
    workflow.add_node("fetch_articles", fetch_articles_node)
    workflow.add_node("generate_assets", generate_assets_node)
    workflow.add_node("create_video", create_short_video_node)

    # ã‚¨ãƒƒã‚¸ã®æ¥ç¶š
    workflow.set_entry_point("fetch_articles")
    workflow.add_edge("fetch_articles", "generate_assets")
    workflow.add_edge("generate_assets", "create_video")
    workflow.add_edge("create_video", END)

    return workflow.compile()

```
## main.pyã®ä½œæˆ
```py:main.py
import os
import typer
from typing import Annotated
from graph import create_graph
from state import AgentState
from config import Config

app = typer.Typer()


@app.command()
def generate(
    start_date: Annotated[str, typer.Argument(help="é–‹å§‹æ—¥ (YYYYMMDD)")],
    end_date: Annotated[str, typer.Argument(help="çµ‚äº†æ—¥ (YYYYMMDD)")]
):
    """
    æŒ‡å®šã—ãŸæœŸé–“(YYYYMMDD)ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‹ã‚‰YouTubeã‚·ãƒ§ãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚
    """
    typer.echo(f"ğŸš€ å‡¦ç†ã‚’é–‹å§‹: {start_date} ã‹ã‚‰ {end_date}")

    # ã‚°ãƒ©ãƒ•ã®æ§‹ç¯‰ã¨ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
    graph = create_graph()

    # åˆå›ã‚¹ãƒ†ãƒ¼ãƒˆã®åˆæœŸåŒ–
    run_output_dir = os.path.join(
        Config.OUTPUT_DIR, f"{start_date}_{end_date}"
    )
    os.makedirs(run_output_dir, exist_ok=True)

    initial_state: AgentState = {
        "start_date": start_date,
        "end_date": end_date,
        "run_output_dir": run_output_dir,
        "articles": [],
        "image_prompts": [],
        "audio_paths": [],
        "image_paths": [],
        "script_paths": [],
        "video_path": None,
        "error": None
    }

    # LangGraphã®å®Ÿè¡Œ
    try:
        for output in graph.stream(initial_state):
            for node_name, state_update in output.items():
                typer.echo(f"âœ… Node [{node_name}] ãŒå®Œäº†ã—ã¾ã—ãŸ")

        typer.echo(f"âœ¨ å…¨å·¥ç¨‹ãŒå®Œäº†ã—ã¾ã—ãŸï¼ output/ ãƒ•ã‚©ãƒ«ãƒ€ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    except Exception as e:
        typer.secho(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", fg=typer.colors.RED)


if __name__ == "__main__":
    app()


```
## å‹•ä½œãƒã‚§ãƒƒã‚¯
```bash
uv run 20260212 20260217
```
## å‹•ç”»ç·¨é›†
### Canva
### ç´ æã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
ç”Ÿæˆã—ãŸå‹•ç”»ã¨éŸ³å£°ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™
### å‹•ç”»ã¨éŸ³å£°ã‚’åˆæˆ
ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã«å‹•ç”»ã¨éŸ³å£°ã‚’ç½®ãã¾ã™
### æ–‡å­—ã®æŒ¿å…¥
ãƒ­ã‚°ã«ã‚ã‚‹æ–‡è¨€ã‚’å‹•ç”»ã«å…¥ã‚Œã¾ã™ã€‚
### BGMã¨SEã®åˆæˆ
BGMã¨SEã‚’å…¥ã‚Œã¾ã™ã€‚
###

## Youtubeã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
## ã¾ã¨ã‚
