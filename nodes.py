import base64
import json
import os
import random
import re
from datetime import datetime
import azure.cognitiveservices.speech as speechsdk
from openai import AzureOpenAI
from moviepy.video.VideoClip import ImageClip
from moviepy.audio.io.AudioFileClip import AudioFileClip
from moviepy.audio.AudioClip import concatenate_audioclips
from moviepy.video.compositing.CompositeVideoClip import concatenate_videoclips
from moviepy.video.fx.Resize import Resize
from moviepy.video.io.VideoFileClip import VideoFileClip
from config import Config
from state import AgentState


def _log_node_output(run_dir: str, node_name: str, payload: dict):
    """Append a JSON line containing node metadata to the current run directory."""
    os.makedirs(run_dir, exist_ok=True)
    log_path = os.path.join(run_dir, "node_logs.jsonl")
    entry = {
        "timestamp": datetime.now().isoformat(timespec="seconds"),
        "node": node_name,
        "payload": payload
    }
    with open(log_path, "a", encoding="utf-8") as log_file:
        json.dump(entry, log_file, ensure_ascii=False)
        log_file.write("\n")


def _extract_title_from_content(raw_content: str, fallback: str) -> str:
    """Return the first non-empty line from raw markdown as a human friendly title."""
    for line in raw_content.splitlines():
        candidate = line.strip().lstrip("#").strip()
        if candidate:
            return candidate[:120]
    return fallback


text_client = AzureOpenAI(
    api_key=Config.AZURE_TEXT_API_KEY,
    api_version=Config.AZURE_TEXT_API_VERSION,
    azure_endpoint=Config.AZURE_TEXT_ENDPOINT,
    azure_deployment=Config.AZURE_OPENAI_DEPLOYMENT_NAME,
)

image_client = AzureOpenAI(
    api_key=Config.AZURE_IMAGE_API_KEY,
    api_version=Config.AZURE_IMAGE_API_VERSION,
    azure_endpoint=Config.AZURE_IMAGE_ENDPOINT,
    azure_deployment=Config.AZURE_IMAGE_DEVELOPMENT_NAME,
)

VIDEO_EXTENSIONS = (".mp4", ".mov", ".m4v", ".avi", ".webm", ".mkv")


def _list_movie_files() -> list[str]:
    """Collect usable background video files from the configured movie directory."""
    movie_dir = Config.MOVIE_DIR
    if not movie_dir or not os.path.isdir(movie_dir):
        return []
    files = []
    for name in os.listdir(movie_dir):
        if name.lower().endswith(VIDEO_EXTENSIONS):
            files.append(os.path.join(movie_dir, name))
    return files


def _format_date_label(date_str: str) -> str:
    """Convert YYYYMMDD into YYYY/MM/DD for display."""
    try:
        return datetime.strptime(date_str, "%Y%m%d").strftime("%Y/%m/%d")
    except ValueError:
        return date_str


def _format_date_range_label(start: str, end: str) -> str:
    """Create a human-friendly date range label."""
    start_label = _format_date_label(start)
    end_label = _format_date_label(end)
    return start_label if start == end else f"{start_label} - {end_label}"


def _clean_hashtag_text(text: str) -> str:
    """Normalize text so it can sit behind a YouTube hashtag."""
    cleaned = re.sub(r"[#ÔºÉ]", "", text)
    cleaned = re.sub(r"\s+", "", cleaned)
    cleaned = re.sub(r"[^\w„ÅÅ-„Çì„Ç°-„É∂‰∏Ä-ÈæØ„Éº]+", "", cleaned)
    return cleaned[:20].strip()


def _extract_hashtags(articles: list[dict]) -> list[str]:
    """Build a short list of hashtags derived from article titles."""
    hashtags: list[str] = []
    for article in articles:
        candidate = article.get('display_title') or article.get('title')
        if not candidate:
            continue
        cleaned = _clean_hashtag_text(candidate)
        if not cleaned or cleaned in hashtags:
            continue
        hashtags.append(cleaned)
        if len(hashtags) >= 5:
            break

    for fallback in ("„Éã„É•„Éº„Çπ", "„Ç∑„Éß„Éº„ÉàÂãïÁîª", "AIÈÄüÂ†±"):
        if len(hashtags) >= 5:
            break
        if fallback not in hashtags:
            hashtags.append(fallback)

    return hashtags[:5]


def _generate_youtube_metadata(state: AgentState) -> dict:
    """Compose a YouTube-ready title, description, and hashtags from the run state."""
    articles = state.get('articles', [])
    range_label = _format_date_range_label(
        state['start_date'], state['end_date'])

    if articles:
        title = f"{range_label}„ÅÆ‰∏ªË¶Å„Éã„É•„Éº„ÇπTOP{len(articles)} | „Ç∑„Éß„Éº„ÉàËß£Ë™¨"
        if len(articles) == 1:
            title = f"{range_label} {articles[0]['title'] } | „Éã„É•„Éº„Çπ„Ç∑„Éß„Éº„Éà"
    else:
        title = f"{range_label}„ÅÆ„Éã„É•„Éº„Çπ„ÉÄ„Ç§„Ç∏„Çß„Çπ„Éà"

    description_lines = [
        f"üìÖ ÂèéÈå≤ÊúüÈñì: {range_label}",
        "",
        "üìù Âèñ„Çä‰∏ä„Åí„Åü„Éà„Éî„ÉÉ„ÇØ:"
    ]
    if articles:
        for article in articles:
            description_lines.append(
                f"- {article.get('display_title') or article['title']} ({_format_date_label(article.get('date', state['start_date']))})"
            )
    else:
        description_lines.append("- Ë©≤ÂΩì„Åô„ÇãË®ò‰∫ã„ÅØË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì„Åß„Åó„Åü„ÄÇ")

    description = "\n".join(description_lines).strip()
    hashtags = _extract_hashtags(articles)
    if articles:
        primary = articles[0]
        base_thumb = primary.get('display_title') or primary.get('title') or ""
    else:
        base_thumb = ""
    if base_thumb:
        thumbnail_title = base_thumb[:40]
    else:
        thumbnail_title = title[:40] if title else "ÊúÄÊñ∞„Éã„É•„Éº„Çπ"

    return {
        "title": title,
        "description": description,
        "hashtags": hashtags,
        "thumbnail_title": thumbnail_title
    }


def _format_timestamp(seconds: float) -> str:
    """Return SBV timestamp (H:MM:SS.mmm)."""
    milliseconds = max(0, int(round(seconds * 1000)))
    hours, remainder = divmod(milliseconds, 3600 * 1000)
    minutes, remainder = divmod(remainder, 60 * 1000)
    secs, millis = divmod(remainder, 1000)
    return f"{hours}:{minutes:02}:{secs:02}.{millis:03}"


def _split_sentences_for_captions(text: str) -> list[str]:
    """Split narration text into SBV-friendly chunks."""
    if not text:
        return []

    stripped = text.strip()
    if not stripped:
        return []

    # „Äå„ÄÇ„Äç„Åß„ÅÆ„ÅøÂå∫Âàá„Çä„ÄÅÂè•Ë™≠ÁÇπ„ÇíÁ∂≠ÊåÅ„Åó„Åü„Åæ„ÅæÊäΩÂá∫
    sentences = [
        chunk.strip()
        for chunk in re.findall(r'[^„ÄÇ]+„ÄÇ?', stripped)
        if chunk.strip()
    ]

    return sentences or [stripped]


def _build_sbv_caption(text: str, duration: float | None) -> str:
    """Generate SBV caption text with pseudo-timed segments."""
    sentences = _split_sentences_for_captions(text)
    if not sentences:
        sentences = ["ÔºàÂÜÖÂÆπ„Å™„ÅóÔºâ"]

    total_chars = sum(len(s) for s in sentences) or 1
    total_duration = duration if duration and duration > 0 else len(
        sentences) * 3.0

    raw_durations = []
    for sentence in sentences:
        portion = max(0.8, (len(sentence) / total_chars) * total_duration)
        raw_durations.append(portion)

    scale = total_duration / \
        sum(raw_durations) if sum(raw_durations) > 0 else 1.0
    durations = [d * scale for d in raw_durations]

    lines = []
    cursor = 0.0
    for sentence, seg_duration in zip(sentences, durations):
        start_ts = _format_timestamp(cursor)
        end_ts = _format_timestamp(cursor + seg_duration)
        lines.append(f"{start_ts},{end_ts}")
        lines.append(sentence)
        lines.append("")
        cursor += seg_duration

    return "\n".join(lines).strip() + "\n"


def fetch_articles_node(state: AgentState):
    """Load dated markdown articles in range and summarize them for narration."""
    target_articles = []
    start = datetime.strptime(state['start_date'], "%Y%m%d")
    end = datetime.strptime(state['end_date'], "%Y%m%d")
    run_dir = state.get('run_output_dir') or Config.OUTPUT_DIR

    single_article_path = state.get("single_article_path")

    if not single_article_path and not os.path.exists(Config.ARTICLE_DIR):
        os.makedirs(Config.ARTICLE_DIR)

    # 1. „Éï„Ç°„Ç§„É´„ÅÆ„Éï„Ç£„É´„Çø„É™„É≥„Ç∞
    files_to_process: list[tuple[str, str, str]] = []
    if single_article_path:
        if not os.path.isfile(single_article_path):
            raise FileNotFoundError(f"Ë®ò‰∫ã„Éï„Ç°„Ç§„É´„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì: {single_article_path}")
        basename = os.path.basename(single_article_path)
        match = re.match(r"(\d{8})_(.*)\.md", basename)
        if match:
            file_date_str, title = match.groups()
        else:
            file_date_str = state['start_date']
            title = os.path.splitext(basename)[0]
        files_to_process.append((single_article_path, title, file_date_str))
    else:
        for filename in os.listdir(Config.ARTICLE_DIR):
            match = re.match(r"(\d{8})_(.*)\.md", filename)
            if match:
                file_date_str, title = match.groups()
                file_date = datetime.strptime(file_date_str, "%Y%m%d")
                if start <= file_date <= end:
                    files_to_process.append(
                        (os.path.join(Config.ARTICLE_DIR, filename), title, file_date_str))

    # 2. ÂêÑË®ò‰∫ã„ÅÆË™≠„ÅøËæº„Åø„Å®Ë¶ÅÁ¥ÑÔºà„Éä„É¨„Éº„Ç∑„Éß„É≥ÂéüÁ®ø‰ΩúÊàêÔºâ
    for filepath, title, date_str in files_to_process:
        with open(filepath, 'r', encoding='utf-8') as f:
            raw_content = f.read()

        # GPT-4o„Å´„Çà„ÇãË¶ÅÁ¥Ñ„Å®„Éä„É¨„Éº„Ç∑„Éß„É≥Êï¥ÂΩ¢
        # „Åì„Åì„ÅßURL„ÅÆÈô§Âéª„ÇÑËá™ÁÑ∂„Å™Ë®Ä„ÅÑÂõû„Åó„Å∏„ÅÆÂ§âÊèõ„ÇíÊåáÁ§∫
        response = text_client.chat.completions.create(
            model=Config.AZURE_OPENAI_DEPLOYMENT_NAME,  # GPT-4oÁî®„Éá„Éó„É≠„Ç§Âêç
            messages=[
                {"role": "system", "content": "„ÅÇ„Å™„Åü„ÅØÂÑ™ÁßÄ„Å™„Éã„É•„Éº„Çπ„Ç¢„Éä„Ç¶„É≥„Çµ„Éº„Åß„Åô„ÄÇ"},
                {"role": "user", "content": f"""
‰ª•‰∏ã„ÅÆ„Éã„É•„Éº„ÇπË®ò‰∫ã„Çí„ÄÅYouTube„Ç∑„Éß„Éº„ÉàÁî®„ÅÆ„Éä„É¨„Éº„Ç∑„Éß„É≥ÂéüÁ®ø„Å´Ë¶ÅÁ¥Ñ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

„ÄêÂà∂Á¥Ñ‰∫ãÈ†Ö„Äë
„Éª500ÊñáÂ≠ó‰ª•ÂÜÖ
„ÉªÂÜíÈ†≠„ÅßÊ†∏ÂøÉ„Çí‰ºù„Åà„ÇãÔºö„Äå‰Ωï„ÅåËµ∑„Åç„Åü„ÅÆ„Åã„Äç„Äå„Å™„ÅúÈáçË¶Å„Å™„ÅÆ„Åã„Äç„ÇíÊúÄÂàù„Å´ÊòéÁ§∫
„Éª„Åù„ÅÆÂæå„ÄÅÊôÇÁ≥ªÂàó„ÇÑÂõ†ÊûúÈñ¢‰øÇ„Å´Ê≤ø„Å£„Å¶ËÉåÊôØ„ÉªÁµåÁ∑Ø„ÉªÂΩ±Èüø„ÇíÁ∞°ÊΩî„Å´Ë™¨Êòé
„ÉªURL„ÇÑË®òÂè∑Ôºà[ ]„ÄÅ( )„Å™„Å©Ôºâ„ÅØÂÆåÂÖ®„Å´ÂâäÈô§„Åæ„Åü„ÅØËá™ÁÑ∂„Å™Ë®ÄËëâ„Å´ÁΩÆ„ÅçÊèõ„Åà„Çã
„ÉªÂ∞ÇÈñÄÁî®Ë™û„ÅØ‰ΩøÁî®ÂèØÔºà„Éì„Ç∏„Éç„Çπ„Éë„Éº„ÇΩ„É≥Âêë„ÅëÔºâ„Å†„Åå„ÄÅÂøÖË¶Å„Å´Âøú„Åò„Å¶Á∞°ÊΩî„Å™Ë£úË∂≥„ÇíÂÖ•„Çå„Çã
„Éª„Åß„Åô„Éª„Åæ„ÅôË™ø„ÅßÁµ±‰∏Ä„Åó„ÄÅÈÅ†Âõû„Åó„Å™Ë°®Áèæ„ÅØÈÅø„Åë„Çã
„Éª„Éì„Ç∏„Éç„Çπ„Å∏„ÅÆÂΩ±Èüø„ÇÑÂÆüÂãôÁöÑ„Å™ÊÑèÂë≥„ÇíÂÑ™ÂÖàÁöÑ„Å´Âê´„ÇÅ„Çã

Ë®ò‰∫ã„Çø„Ç§„Éà„É´: {title}
Ë®ò‰∫ãÂÜÖÂÆπ:
{raw_content}
"""}
            ]
        )

        summarized_content = response.choices[0].message.content.strip()
        human_title = _extract_title_from_content(
            raw_content, title.replace("_", " "))

        target_articles.append({
            'title': title,
            'display_title': human_title,
            'content': summarized_content,  # „Åì„Åì„Å´Á∂∫È∫ó„Å™Ë¶ÅÁ¥Ñ„ÅåÂÖ•„Çã
            'date': date_str
        })
        print(f"‚úÖ Ë¶ÅÁ¥ÑÂÆå‰∫Ü: {title}")

    _log_node_output(
        run_dir,
        "fetch_articles",
        {
            "article_count": len(target_articles),
            "article_titles": [article['display_title'] for article in target_articles],
            "articles": target_articles
        }
    )

    return {'articles': target_articles, 'run_output_dir': run_dir}


def generate_audio_assets_node(state: AgentState):
    """Create narration audio files and scripts for each article."""
    audio_paths = []
    script_paths = []
    voice_outputs = []

    run_dir = state.get('run_output_dir') or Config.OUTPUT_DIR
    os.makedirs(run_dir, exist_ok=True)

    for i, article in enumerate(state['articles']):
        # ÂêÑË®ò‰∫ã„Åî„Å®„Å´ Azure Speech „ÇíË®≠ÂÆöÔºàÈü≥Â£∞„Çπ„Çø„Ç§„É´„ÇíÁµ±‰∏ÄÔºâ
        speech_config = speechsdk.SpeechConfig(
            subscription=Config.AZURE_SPEECH_KEY,
            region=Config.AZURE_SPEECH_REGION
        )
        speech_config.speech_synthesis_voice_name = "ja-JP-NanamiNeural"

        audio_filename = f"audio_{i}.wav"
        audio_path = os.path.join(run_dir, audio_filename)
        audio_config = speechsdk.audio.AudioOutputConfig(filename=audio_path)

        # „Éä„É¨„Éº„Ç∑„Éß„É≥„ÇíÈü≥Â£∞Âåñ„Åó„ÄÅ„Éï„Ç°„Ç§„É´„Å∏‰øùÂ≠ò
        synthesizer = speechsdk.SpeechSynthesizer(
            speech_config=speech_config, audio_config=audio_config)
        synthesizer.speak_text_async(article['content']).get()
        audio_paths.append(audio_path)

        # Â≠óÂπï„ÅÆ„Çø„Ç§„Éü„É≥„Ç∞Ë®àÁÆóÁî®„Å´Èï∑„Åï„ÇíÂèñÂæóÔºàÂ§±Êïó„Åó„Å¶„ÇÇÁÑ°Ë¶ñÔºâ
        audio_duration = None
        try:
            temp_clip = AudioFileClip(audio_path)
            audio_duration = temp_clip.duration or None
        except Exception:
            audio_duration = None
        finally:
            try:
                temp_clip.close()
            except Exception:
                pass

        script_filename = f"script_{i}.sbv"
        script_path = os.path.join(run_dir, script_filename)
        # SBV ÂΩ¢Âºè„ÅÆÂ≠óÂπï„ÇíÁîüÊàê„Åó„ÄÅYouTube„ÅßÁõ¥Êé•‰Ωø„Åà„Çã„Çà„ÅÜ„Å´„Åô„Çã
        captions_content = _build_sbv_caption(
            article['content'], audio_duration)
        with open(script_path, "w", encoding="utf-8") as script_file:
            script_file.write(captions_content)
        script_paths.append(script_path)

        voice_outputs.append({
            "index": i,
            "article_title": article.get('display_title') or article['title'],
            "audio_path": audio_path,
            "script_path": script_path,
            "spoken_text": article['content']
        })

    _log_node_output(
        run_dir,
        "generate_audio_assets",
        {
            "audio_files": [os.path.basename(p) for p in audio_paths],
            "script_files": [os.path.basename(p) for p in script_paths],
            "voice_outputs": [
                {
                    "index": entry["index"],
                    "article_title": entry["article_title"],
                    "spoken_text": entry["spoken_text"],
                    "audio_file": os.path.basename(entry["audio_path"]),
                    "script_file": os.path.basename(entry["script_path"])
                }
                for entry in voice_outputs
            ]
        }
    )

    return {
        'audio_paths': audio_paths,
        'script_paths': script_paths,
        'run_output_dir': run_dir
    }


def generate_visual_assets_node(state: AgentState):
    """Create illustrative prompts and images for each article."""
    image_paths = []
    image_prompts = []
    image_outputs = []

    run_dir = state.get('run_output_dir') or Config.OUTPUT_DIR
    os.makedirs(run_dir, exist_ok=True)

    for i, article in enumerate(state['articles']):
        # GPT „Å´Êò†ÁîªÁöÑ„Å™È¢®ÊôØ„Éó„É≠„É≥„Éó„Éà„Çí‰Ωú„Çâ„Åõ„ÇãÔºàFLUXÁî®Ôºâ
        prompt_response = text_client.chat.completions.create(
            model=Config.AZURE_OPENAI_DEPLOYMENT_NAME,
            messages=[
                {
                    "role": "system",
                    "content": (
                        "„ÅÇ„Å™„Åü„ÅØÂ†±ÈÅì„Éì„Ç∏„É•„Ç¢„É´„ÅÆ„Ç≥„É≥„Çª„Éó„Éà„Ç¢„Éº„ÉÜ„Ç£„Çπ„Éà„Åß„Åô„ÄÇ‰ª•‰∏ã„ÅÆ„Éã„É•„Éº„ÇπË®ò‰∫ã„ÇíË™≠„Åø„ÄÅ"
                        "„Åù„ÅÆÂÜÖÂÆπ„ÇíË¶ñË¶öÁöÑ„Å´‰ºù„Åà„ÇãÁîªÂÉèÁîüÊàêÁî®„Éó„É≠„É≥„Éó„Éà„ÇíËã±Ë™û„Åß‰ΩúÊàê„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n\n"

                        "„ÄêÂøÖÈ†àË¶ÅÁ¥†„Äë\n"
                        "„Éª„Éã„É•„Éº„Çπ„ÅÆÊ†∏ÂøÉÁöÑ„Å™„Äå„É¢„Éé„Äç„ÄåÂ†¥ÊâÄ„Äç„ÄåÁä∂Ê≥Å„Äç„ÇíÂÖ∑‰ΩìÁöÑ„Å´ÊèèÂÜô\n"
                        "„ÉªÊäÄË°ìÁ≥ª„Éã„É•„Éº„ÇπÔºöË£ΩÂìÅ„ÄÅ„Éá„Éê„Ç§„Çπ„ÄÅ„Ç§„É≥„Éï„É©„ÄÅ„Éá„Ç∏„Çø„É´„Ç§„É≥„Çø„Éº„Éï„Çß„Éº„Çπ\n"
                        "„Éª„Éì„Ç∏„Éç„ÇπÁ≥ª„Éã„É•„Éº„ÇπÔºö„Ç™„Éï„Ç£„ÇπÁ©∫Èñì„ÄÅÈÉΩÂ∏ÇÊôØË¶≥„ÄÅ‰ºÅÊ•≠„É≠„Ç¥„ÅÆ„Å™„ÅÑ„Éì„É´Áæ§\n"
                        "„ÉªÊîøÁ≠ñÁ≥ª„Éã„É•„Éº„ÇπÔºöË≠∞Â†¥„ÄÅÂÖ¨ÂÖ±ÊñΩË®≠„ÄÅË±°Âæ¥ÁöÑ„Å™Âª∫ÈÄ†Áâ©\n"
                        "„ÉªÁí∞Â¢ÉÁ≥ª„Éã„É•„Éº„ÇπÔºöËá™ÁÑ∂Áí∞Â¢É„ÄÅÊ∞óÂÄôÁèæË±°„ÄÅ„Ç®„Ç≥„Ç∑„Çπ„ÉÜ„É†\n\n"

                        "„ÄêÁ¶ÅÊ≠¢‰∫ãÈ†Ö„Äë\n"
                        "„ÉªÂÆüÂú®„ÅÆ‰∫∫Áâ©„ÅÆÈ°î„ÇÑ‰ΩìÔºàÂæå„ÇçÂßø„ÇÑÈÅ†ÊôØ„ÅÆ„Ç∑„É´„Ç®„ÉÉ„Éà„ÅØÂèØÔºâ\n"
                        "„ÉªÂÆüÂú®‰ºÅÊ•≠„ÅÆ„É≠„Ç¥„ÇÑÂïÜÊ®ô\n"
                        "„ÉªÁâπÂÆöÂèØËÉΩ„Å™ÂÄã‰∫∫„ÅåÂÜô„ÇäËæº„ÇÄÊßãÂõ≥\n\n"

                        "„ÄêÊé®Â•®Ë°®Áèæ„Äë\n"
                        "„ÉªÊäΩË±°ÁöÑ„Å™„Éì„Ç∏„É•„Ç¢„É´„É°„Çø„Éï„Ç°„ÉºÔºà‰æãÔºöAI„Éã„É•„Éº„Çπ‚ÜíËÑ≥„ÅÆ„Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÊ®°ÊßòÔºâ\n"
                        "„ÉªË±°Âæ¥ÁöÑ„Å™„Ç™„Éñ„Ç∏„Çß„ÇØ„ÉàÔºà‰æãÔºöÂçäÂ∞é‰Ωì„Éã„É•„Éº„Çπ‚Üí„Éû„Ç§„ÇØ„É≠„ÉÅ„ÉÉ„Éó„ÅÆ„ÇØ„É≠„Éº„Ç∫„Ç¢„ÉÉ„ÉóÔºâ\n"
                        "„ÉªÁí∞Â¢É„ÇÑÁ©∫Èñì„ÅßÁä∂Ê≥Å„ÇíË°®ÁèæÔºà‰æãÔºöÁµåÊ∏àÂç±Ê©ü‚ÜíÁÑ°‰∫∫„ÅÆ„Ç™„Éï„Ç£„Çπ„Éï„É≠„Ç¢Ôºâ\n\n"

                        "„Äê„Çπ„Çø„Ç§„É´ÊåáÂÆö„Äë\n"
                        "„Éª9:16Á∏¶ÂûãÊßãÂõ≥„ÇíÊÑèË≠ò\n"
                        "„Éª„Éì„Ç∏„É•„Ç¢„É´„Ç∏„É£„Éº„Éä„É™„Ç∫„É†È¢®„ÅÆÂÜôÂÆüÁöÑ„Çπ„Çø„Ç§„É´\n"
                        "„ÉªËâ≤ÂΩ©„ÅØË®ò‰∫ã„ÅÆ„Éà„Éº„É≥ÔºàÂ∏åÊúõÁöÑ/Ë≠¶ÂëäÁöÑ/‰∏≠Á´ãÁöÑÔºâ„Å´Âêà„Çè„Åõ„Çã\n"
                        "„ÉªË¶ñË™çÊÄß„ÅÆÈ´ò„ÅÑÊòéÁû≠„Å™ÊßãÂõ≥"
                    )
                },
                {
                    "role": "user",
                    "content": f"Ë®ò‰∫ã„Çø„Ç§„Éà„É´: {article.get('display_title') or article['title']}\nË®ò‰∫ãÂÜÖÂÆπ: {article['content']}"
                }
            ]
        )
        img_prompt = prompt_response.choices[0].message.content.strip()
        image_prompts.append(img_prompt)

        # 9:16 ÊØîÁéá„ÅÆ„Ç§„É©„Çπ„Éà„ÇíÊèèÁîª„Åó„ÄÅÂãïÁîªÂÜíÈ†≠„ÅÆÈùôÊ≠¢Áîª„Å´‰ΩøÁî®
        image_result = image_client.images.generate(
            model=Config.AZURE_IMAGE_DEVELOPMENT_NAME,
            prompt=f"{img_prompt} Digital art style, vibrant colors, 9:16 aspect ratio focus.",
            size="1792x1024",
            n=1,
            response_format="b64_json",
        )

        image_data = image_result.data[0]
        image_location = None
        if image_data.url:
            image_location = image_data.url
            image_paths.append(image_location)
        else:
            image_filename = f"image_{i}.png"
            image_path = os.path.join(run_dir, image_filename)
            with open(image_path, "wb") as img_file:
                img_file.write(base64.b64decode(image_data.b64_json))
            image_location = image_path
            image_paths.append(image_location)

        image_outputs.append({
            "index": i,
            "article_title": article.get('display_title') or article['title'],
            "prompt": img_prompt,
            "image_path": image_location
        })

    _log_node_output(
        run_dir,
        "generate_visual_assets",
        {
            "image_files": [os.path.basename(p) if p else None for p in image_paths],
            "image_prompts": image_prompts,
            "image_outputs": [
                {
                    "index": entry["index"],
                    "article_title": entry["article_title"],
                    "prompt": entry["prompt"],
                    "image_file": os.path.basename(entry["image_path"]) if entry["image_path"] else None
                }
                for entry in image_outputs
            ]
        }
    )

    return {
        'image_paths': image_paths,
        'run_output_dir': run_dir
    }


def create_short_video_node(state: AgentState):
    """Combine generated images, stock footage, and narration into one short video."""
    clips = []
    run_dir = state.get('run_output_dir') or Config.OUTPUT_DIR
    os.makedirs(run_dir, exist_ok=True)
    output_path = os.path.join(run_dir, "final_youtube_short.mp4")
    movie_files = _list_movie_files()
    video_sources: list[VideoFileClip] = []
    article_visual_logs = []
    article_audio_clips: list[AudioFileClip] = []
    audio_timeline = []
    audio_cursor = 0.0

    for i, article in enumerate(state['articles']):
        audio = AudioFileClip(state['audio_paths'][i])
        article_audio_clips.append(audio)
        duration = audio.duration or 0
        duration = max(duration, 0.001)

        base_image = ImageClip(state['image_paths'][i], duration=duration)
        base_image = base_image.with_effects([Resize(height=1920)])

        def zoom_factor(t, total=duration):
            return 1 + 0.05 * (t / max(total, 0.001))

        base_image = base_image.with_effects([Resize(new_size=zoom_factor)])
        base_image = base_image.with_position("center")

        segments = []
        movie_segments_log = []

        image_intro_duration = min(5, duration)
        segments.append(base_image.with_duration(image_intro_duration))
        remaining = duration - image_intro_duration

        while remaining > 1e-3 and movie_files:
            movie_path = random.choice(movie_files)
            try:
                video_clip = VideoFileClip(movie_path)
                video_sources.append(video_clip)
            except Exception:
                continue

            clip_duration = min(5, remaining, video_clip.duration or 0)
            if clip_duration <= 0:
                video_clip.close()
                continue

            max_start = max(
                0, (video_clip.duration or clip_duration) - clip_duration)
            start = random.uniform(0, max_start) if max_start > 0 else 0

            segment = video_clip.subclipped(
                start_time=start,
                end_time=start + clip_duration
            ).with_audio(None)

            segment = segment.with_effects(
                [Resize(height=1920)]).with_position("center")
            segments.append(segment)

            movie_segments_log.append({
                "file": os.path.basename(movie_path),
                "start": round(start, 2),
                "duration": round(clip_duration, 2)
            })

            remaining -= clip_duration

        if remaining > 1e-3:
            segments.append(base_image.with_duration(remaining))

        article_video = concatenate_videoclips(
            segments, method="compose").with_duration(duration)
        article_video.audio = audio
        clips.append(article_video)

        article_visual_logs.append({
            "article": article.get('display_title') or article['title'],
            "movie_segments": movie_segments_log
        })

        audio_timeline.append({
            "article": article.get('display_title') or article['title'],
            "audio_file": os.path.basename(state['audio_paths'][i]),
            "start": round(audio_cursor, 2),
            "duration": round(duration, 2)
        })
        audio_cursor += duration

    final_video = concatenate_videoclips(clips, method="compose")
    final_audio = None
    if article_audio_clips:
        final_audio = concatenate_audioclips(article_audio_clips)
        final_video.audio = final_audio

    try:
        final_video.write_videofile(
            output_path, fps=24, codec="libx264", audio_codec="aac"
        )
    finally:
        final_video.close()
        if final_audio is not None:
            try:
                final_audio.close()
            except Exception:
                pass
        for audio_clip in article_audio_clips:
            try:
                audio_clip.close()
            except Exception:
                pass
        for source in video_sources:
            try:
                source.close()
            except Exception:
                pass

    _log_node_output(
        run_dir,
        "create_video",
        {
            "video_file": os.path.basename(output_path),
            "clip_count": len(clips),
            "articles": article_visual_logs,
            "audio_timeline": audio_timeline
        }
    )

    return {
        "video_path": output_path,
        'run_output_dir': run_dir
    }


def generate_youtube_metadata_node(state: AgentState):
    """Produce a YouTube-ready title, description, and hashtag set."""
    run_dir = state.get('run_output_dir') or Config.OUTPUT_DIR
    os.makedirs(run_dir, exist_ok=True)

    metadata = _generate_youtube_metadata(state)
    hashtags_line = " ".join(f"#{tag}" for tag in metadata['hashtags']).strip()
    metadata_path = os.path.join(run_dir, "youtube_meta.txt")

    articles = state.get("articles") or []
    if articles:
        primary_title = articles[0].get(
            'display_title') or articles[0].get('title') or "ÊúÄÊñ∞„Éã„É•„Éº„Çπ"
    else:
        primary_title = "ÊúÄÊñ∞„Éã„É•„Éº„Çπ"
    note_line = f"- note: {primary_title}"
    zenn_line = f"- zenn: {primary_title}"

    spoken_block = "\n\n".join(
        article.get('content', '').strip()
        for article in articles if article.get('content')
    ).strip()

    sections = [
        "„ÄêÂÜÖÂÆπ„Äë",
        note_line,
        zenn_line,
        "",
    ]

    if articles:
        for idx, article in enumerate(articles, start=1):
            title = article.get('display_title') or article.get(
                'title') or f"Ë®ò‰∫ã{idx}"
            spoken = article.get('content', '').strip()
            sections.append(f"{idx}. {title}")
            if spoken:
                sections.append(spoken)
            sections.append("")
    elif spoken_block:
        sections.append(spoken_block)
        sections.append("")

    sections.extend([
        "ITÁ≥ª„ÅÆÊÉÖÂ†±„ÇíÁô∫‰ø°„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ",
        "note",
        "https://note.com/kenquichi",
        "zenn",
        "https://zenn.dev/kenquichi",
        "",
        "„Ç®„É≥„Ç∏„Éã„Ç¢„Å´„Å™„ÇãË¨õÂ∫ß",
        "https://note.com/kenquichi/m/mc4926a77c1da",
        "",

        "\n".join(f"#{tag}" for tag in metadata['hashtags']) or "#„Éã„É•„Éº„Çπ"
    ])

    final_text = "\n".join(sections).rstrip() + "\n"

    with open(metadata_path, "w", encoding="utf-8") as meta_file:
        meta_file.write(final_text)

    _log_node_output(
        run_dir,
        "generate_youtube_metadata",
        {
            "metadata_file": os.path.basename(metadata_path),
            "title": metadata['title'],
            "description": metadata['description'],
            "hashtags": metadata['hashtags'],
            "hashtags_line": hashtags_line or "#„Éã„É•„Éº„Çπ #„Ç∑„Éß„Éº„ÉàÂãïÁîª",
            "thumbnail_file": os.path.basename(state.get('thumbnail_path')) if state.get('thumbnail_path') else None,
            "thumbnail_title": metadata['thumbnail_title']
        }
    )

    return {
        "youtube_metadata_path": metadata_path,
        "thumbnail_title": metadata['thumbnail_title'],
        'run_output_dir': run_dir
    }
